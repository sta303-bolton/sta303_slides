---
title: "STA303H1S/1002H Week 11"
subtitle: "Wham! GAM! Thank you, mam! Generalized additive models"
author: "Prof. Liza Bolton"
date: "Mar 29, 2021"
output:
  xaringan::moon_reader:
    css: "slides_303.css"
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1,
  crayon.enabled = FALSE
  )
# Set dpi and height for images
library(knitr)
opts_chunk$set(fig.height = 2.65, dpi =300, warning=FALSE, message=FALSE) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")

library(tidyverse)
library(gridExtra)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
### xaringan::inf_mr() -> use this for display within Rstudio

xaringanExtra::use_share_again()
xaringanExtra::style_share_again(
  share_buttons = "none"
)

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

name: topics

## Topics
You can click the following links to navigate through the slides (in the HTML version).

* [Admin notes](#admin)
* [Introduction to Generalized Additive Models](#gams)
* [Case study: Cherry tree](#cherry)
* [Case study: Portuguese larks](#larks)
* [Further comments on GAMs](#final)

---
class: middle, center, inverse
name: admin
# Admin notes

.header[<u>[Go to topics list](#topics)</u>]

---
## Weekly check-in (week 10)

Largely we're looking pretty good! A few clarifications about GLMs and case-control studies that I'll try to hit on in class this week.

## Other

At the time of posting these slides, Amin and I were still finishing up the rubric for the professional development task, but will be aiming to post it Tuesday. As said before, the main things you need to know were described in last Monday's announcement.

---
## Upcoming assessments: weekly 

* [LAST weekly quiz](https://q.utoronto.ca/courses/204826/quizzes/138615) (.highlight[due Friday, Apr 2 at 6:00 p.m. ET])
* [LAST weekly writing](https://q.utoronto.ca/courses/204826/assignments/506345)
    - Create phase due Apr 1 at 6:00 p.m. ET
    - Assess phase due Apr 2 at 6:00 p.m. ET
    - Reflect phase due Apr 5 at 6:00 p.m. ET
---
## Upcoming assessments: non-weekly 
.midi[

### [Professional development evidence and reflection](https://q.utoronto.ca/courses/204826/assignments/506360) (due Apr 2 at 6:00 p.m. ET)

### [Polished writing 3](https://q.utoronto.ca/courses/204826/assignments/506354) (due Apr 9 at 6:00 p.m. ET)

[Polished writing 3](https://q.utoronto.ca/courses/204826/assignments/506354) must be a response to one of the prompts from Week 8, 9, 10 or 11. You do not have to have completed the activity for that week to be able to submit your response as your polished writing, but the intention is that you are submitting a piece improved based on feedback from your peers.
]

### [Final project](https://q.utoronto.ca/courses/204826/pages/project-overview)
Checklist for this week:
* Get the data for the project: see week 11 class recording
* Draft your research questions
* Explore the data (summaries, basic visualizations)

---
class: middle, center, inverse
name: gams
# Introduction to Generalized Additive Models

.header[<u>[Go to topics list](#topics)</u>]

---

## What are we doing this week?

.large[
- Let's get even more flexible!
    - What are GAMs?
    - Flexibly modelling non-linear behaviour
]

---
## Big picture

```{r, echo=FALSE, fig.align='center', out.width="95%"}
knitr::include_graphics("libs/images/w11/continuum.png")
```

---
## A one tweet GAM lesson

<img src="libs/images/w11/tweet.png" width=700>

[https://twitter.com/ucfagls/status/842444686513991680?ref_src=twsrc%5Etfw](https://twitter.com/ucfagls/status/842444686513991680?ref_src=twsrc%5Etfw)

---
# Non-parametric modelling

A.K.A

- Penalized likelihood
- Smoothing
- Fitting wiggly lines through points
- Semi-parametric models
- Splines

---
## Some fake data

It is very wiggly.

```{r echo=FALSE, fig.height=2.8}
library('mgcv')
set.seed(20)

f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * (10 * x)^3 * (1 - x)^10
ysim <- function(n = 500, scale = 2) {
  x <- runif(n)
  e <- rnorm(n, 0, scale)
  f <- f2(x)
  y <- f + e
  data.frame(y = y, x = x, f2 = f)
}

my_data <- ysim()

p <- ggplot(my_data, aes(x = x, y = y)) +
  geom_point() +
  theme_minimal()

ggsave("libs/images/w11/fake.png", p, width = 7, height = 3.5)
```

```{r, echo=FALSE, fig.align='center', out.width="80%"}
include_graphics("libs/images/w11/fake.png")
```

---
## Linear model?

```{r, fig.height=2.3, fig.show='hide'}
p + geom_smooth(method="lm")
```

```{r, echo=FALSE}
ggsave("libs/images/w11/linear.png", width = 7, height = 3.5)
```
```{r, echo=FALSE, fig.align='center', out.width="80%"}
include_graphics("libs/images/w11/linear.png")
```

Well, that seems bad...

---
### Wouldn't something like this be much nicer?

```{r echo=FALSE, fig.show='hide'}
p + geom_smooth(method="gam", color="purple", formula = y ~ s(x, k = 7))
```

```{r, echo=FALSE}
ggsave("libs/images/w11/nicer.png", width = 7, height = 3.5)
```
```{r, echo=FALSE, fig.align='center', out.width="80%"}
include_graphics("libs/images/w11/nicer.png")
```
That's the goal for this week!

---

## How do we get the wiggles?

.center[
<img src="libs/images/w11/wiggles.jpeg" width =800>
]

---
## How do we get the wiggles?

.large[**Answer: Splines!**]

If you've looked at interference of waves in physics, you'll love this. If you haven't....you'll also love this!

.right-column[<img src = "libs/images/w11/pr.jpeg" width = 300>]
GAMs are both *smooth* and *flexible* thanks to actually being made up of multiple not as flexible functions. _Imagine the Power Rangers robot teaching you a yoga class._

- Each smooth is the sum of a number of **basis functions**
- Each basis function is multiplied by a coefficient
- Each of those coefficients is a parameter of our model

---
## Splines

* Splines are *functions* composed of simpler functions
* Our simpler functions are *basis functions* & the set of basis functions is a *basis*
* When we model using splines, each basis function $b_k$ has a coefficient $\beta_k$
* The resulting spline is a the sum of these weighted basis functions, evaluated at the values of $x$

$$s(x) = \sum_{k = 1}^K \beta_k b_k(x)$$

---
## Picturing basis functions

.midi[
- **Plot a** shows the basis functions of a GAM where all the coefficients are the same.  
- **Plot b** shows the same basis functions *after* model-fitting, where each has a coefficient fit to the data. 
- Basis functions add up to create the overall smooth shape. 

Describing this one, nonlinear relationship (one response and one explanatory variable) requires several parameters, plus an intercept. ]

.center[
<img src="libs/images/w11/basis-functions-nr.png" width=700>
]

.footnote[Image created by [Noam Ross]( https://github.com/noamross/gams-in-r-course/blob/master/images/basis-functions-1.png).]

---
## Wiggle, wiggle, wiggle

```{r, echo=FALSE, fig.align='center', out.width="80%"}
include_graphics("https://github.com/gavinsimpson/intro-gam-webinar-2020/blob/master/resources/basis-fun-anim.gif?raw=true")
```

.footnote[GIF by [Gavin Simpson](https://github.com/gavinsimpson)]

---
### Taking a peak at our coefficients

```{r}
library(mgcv) # you will need to install this
gam_mod <- gam(y ~ s(x, k=7), data=my_data, method="REML")
coef(gam_mod)
```

This is just some meaningless fake data, we'll work through a Case Study more fully.

---
### How many basis functions do we want?


```{r, echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("libs/images/w11/diffbasis-nr.png")
```

.footnote[Image created by [Noam Ross]( https://github.com/noamross/gams-in-r-course/blob/master/images/diffbasis-1.png).]


---
### Smoothing

```{r, echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("libs/images/w11/diffsmooth-1.png")
```

.footnote[Image created by [Noam Ross]( https://github.com/noamross/gams-in-r-course/blob/master/images/diffsmooth-1.png).]

---
.pull-left[### Smoothing

You can think of our fit as being:

$$\text{penalized log lik} = $$
$$\log(\text{Likelihood}) - \lambda \cdot \text{Wiggliness}$$

where $\lambda$ is a smoothing parameter.

We can set lambda with the `sp` (smoothing parameter) option in `gam()` BUT it is recommended that we let R find the best one for us using restricted maximum likelihood ("REML").]

.pull-right[
.large[
```{r, eval=FALSE}
# Sets parameter for the whole model
gam_mod <- gam(y ~ s(x), data=my_data, sp=0.1) 

# Set the parameter for one specific term
gam_mod <- gam(y ~ s(x1, sp=0.1) + s(x2), data=my_data) 

# Let R do it for you - the recommended way
gam_mod <- gam(y ~ s(x), data=my_data, method = "REML")

```
]]

---
## Choices to make: Wiggliness

There are LOTS of ways to pick your wiggle: AIC, generalized cross-validation (GCV, part of the name of `mgcv` and the default for that package), ML and REML. 

## Choices to make: Basis complexity 

We can set a maximum wiggliness by setting a 'size', k, that indicates a maximum number of small functions that could be used to build the model. If we set it bigger than the data, we'll get an error, and if we set it much bigger than needed, it is computationally costly. 

Our effective degrees of freedom (edf) will always be less than k. We can check if we've been sensible in our choice of _k_ with `gam.check()`.

---
## Basis expansions

In the polynomial models we used a polynomial basis expansion of $\boldsymbol{x}$

* $\boldsymbol{x}^0 = \boldsymbol{1}$ &mdash; the model constant term
* $\boldsymbol{x}^1 = \boldsymbol{x}$ &mdash; linear term
* $\boldsymbol{x}^2$
* $\boldsymbol{x}^3$
* &hellip;

So! If the __effective degrees of freedom__ we need for a term is approximately 1, then we're really just smoothing it down to a linear term, the way a covariate would usually enter a model as a fixed effect in our previous models. We may choose to just put it in the model as such, so that we can interpret the coefficient it receives. 


---
## Generalized additive (mixed) models

.pull-left[

We can combine everything we've done in this course so far into generalized additive models (including adding random effects).

\begin{align*}
Y_i \sim & G(\mu_i, \theta)\\
g(\mu_i) = & X_i \beta + Z_iU + f(W_i)
\end{align*}

.midi[
- $Y_i$ are responses
- $G$ is the response distribution
- $X_i$, $Z_i$ and $W_i$ are covariates
- $U$ are our random effects
- $f(w)$ is some sort of wiggly line 
- If we put no restrictions or assumptions on $f$, the estimate $\hat f(w)$ will interpolate the data perfectly (which isn't very interesting)
]
]

__Random effects__

.midi[When fitted with REML or ML, _smooths_ can be viewed as just fancy _random effects_. AND, excitingly, random effects can be viewed as smooths!

If your random effects are fairly simple, you can fit those in `mgcv::gam()` without needing the more complex GAMM functions, like `gamm4::gamm4()`

These two models are equivalent:]

```{r ranefs, eval=FALSE}
m_nlme <- lme4::lmer(travel ~ 1 + (1 | Rail), data = Rail, REML = TRUE) 
m_gam  <- gam(travel ~ s(Rail, bs = "re"), data = Rail, method = "REML")
```

---
## Random effects

The random effect basis, `bs = 're'`, is _not_ as computationally efficient as *lme4* if we have complex random effects terms or even if we just have random effects with many levels (which isn't really that unusual with random effects).

Instead we could use `gamm()` or `gamm4::gamm4()`:

* `gamm()` fits using `lme()`
* `gamm4::gamm4()` fits using `lmer()` or `glmer()`

I.e., you're wanting a response with a conditional distribution that isn't _normal_, use `gamm4::gamm4()`

---
class: middle, center, inverse
name: cherry
# Case study: Cherry tree

.header[<u>[Go to topics list](#topics)</u>]

---
class: middle, center

See the .Rmd in this week's project:  
`devtools::install_github("sta303-bolton/sta303w11")`

---
class: middle, center, inverse
name: larks
# Case study: Portuguese larks

.header[<u>[Go to topics list](#topics)</u>]

---
class: middle, center

See the .Rmd in this week's project:  
`devtools::install_github("sta303-bolton/sta303w11")`

---
class: middle, center, inverse
name: final
# Further comments on GAMs

.header[<u>[Go to topics list](#topics)</u>]

---
## GAMs are models too

How accurate your predictions are depends on how good the model is, as always. (Credit: Eric Pedersen & Gavin Simpson)

```{r misspecify, echo = FALSE, fig.show=FALSE}
set.seed(15)
model_list = c("Appropriate model", 
               "Wrong distribution",
               "Heteroskedasticity",
               "Dependent data",
               "Wrong functional form")
n <- 60
sigma=1
x <- seq(-1,1, length=n)
model_data <- as.data.frame(expand.grid( x=x,model=model_list))
model_data$y <- 5*model_data$x^2 + 2*model_data$x
for(i in model_list){
  if(i == "Appropriate model"){
    model_data[model_data$model==i, "y"] <- model_data[model_data$model==i, "y"]+ 
      rnorm(n,0, sigma)
  } else if(i == "Wrong distribution"){
    model_data[model_data$model==i, "y"] <- model_data[model_data$model==i, "y"]+ 
      rt(n,df = 3)*sigma
  } else if(i == "Heteroskedasticity"){
    model_data[model_data$model==i, "y"] <- model_data[model_data$model==i, "y"]+  
      rnorm(n,0, sigma*10^(model_data[model_data$model==i, "x"]))
  } else if(i == "Dependent data"){
    model_data[model_data$model==i, "y"] <- model_data[model_data$model==i, "y"]+ 
      arima.sim(model = list(ar=c(.7)), n = n,sd=sigma) 
  } else if(i=="Wrong functional form") {
    model_data[model_data$model==i, "y"] <- model_data[model_data$model==i, "y"]+ 
      rnorm(n,0, sigma) + ifelse(model_data[model_data$model==i, "x"]>0, 5,-5)
  }
}
ggplot(aes(x,y), data= model_data)+
  geom_point()+
  geom_line(color=ifelse(model_data$model=="dependent data", "black",NA))+
  facet_wrap(~model)+
  geom_smooth(method=gam, colour="purple", formula = y~s(x,k=12),method.args = list(method="REML"))+
  theme(strip.text = element_text(size=16)) +
  theme_minimal()
```


```{r, echo=FALSE}
ggsave("libs/images/w11/modelstoo.png", width = 6, height = 2.3)
```
```{r, echo=FALSE, fig.align='center', out.width="100%"}
include_graphics("libs/images/w11/modelstoo.png")
```

---
## Select and method
.midi[
.pull-left[
#### Variable selection
Unmodified smoothness selection by GCV, AIC, REML etc. will not usually remove a smooth from a model (not set edf to 0). Most smoothing penalties view the null-space of a smooth as 'completely smooth' and so further penalization does not change it.

With `select = TRUE` we add an extra penalty to the null-space part (the part of the spline that is perfectly smooth).

If you don't have this, smoothness selection can usually only penalize a smooth back to a linear function]

.pull-right[ (because the penalty that's doing smoothness selection only works on the non-smooth (the wiggly) parts of the basis). To perform selection we need to be able to penalize the null space (the smooth parts of the basis) as well.

#### Smoothness selection
The `method` argument to gam selects the smoothness selection criterion. For many practitioners, 'ML' or 'REML' are their default choice, though not the default in `gam()`. Using a likelihood based approach essentially treats the smooth components as random effects.
]]

---
## More conditional distributions than you can shake a squiggly line at

A GAM is just a fancy GLM! So we can fit any of the models we've learned (Poisson, Logistic, Gamma as we saw today in the cherry example). The creators of the `mgcv` package (Simon Wood & colleagues (2016)) have extended the methods to some non-exponential family distributions that are also very helpful, of which we've seen Negative Binomial and Zero-inflated Poisson.

.midi[
.pull-left[
* `binomial()`
* `poisson()`
* `Gamma()`
* `inverse.gaussian()`
* `nb()`
* `tw()`
* `mvn()`
* `multinom()`
]
.pull-right[
* `betar()`
* `scat()`
* `gaulss()`
* `ziplss()`
* `twlss()`
* `cox.ph()`
* `gamals()`
* `ocat()`
]]


---
## A symphony of smoothers

The type of smoother is controlled by the `bs` argument (think *b*asi*s*)

The default is a low-rank thin plate spline `bs = 'tp'`

Many others available (thanks Gavin Simpson for making this list):

.pull-left[

* Cubic splines `bs = 'cr'`
* P splines `bs = 'ps'`
* Cyclic splines `bs = 'cc'` or `bs = 'cp'`
* Adaptive splines `bs = 'ad'`
* Random effect `bs = 're'`
* Factor smooths `bs = 'fs'`
]
.pull-right[
* Duchon splines `bs = 'ds'`
* Spline on the sphere `bs = 'sos'`
* MRFs `bs = 'mrf'`
* Soap-film smooth `bs = 'so'`
* Gaussian process `bs = 'gp'`
]

---

## How do we talk about GAMs?

Presenting results from GAMs is similar to presenting results from other models we've learned except that for smoothed terms we have no single coefficient you can make inference from (i.e. negative, positive, effect size etc.). 

For smoothed variables, we rely a lot on visual methods (e.g. `plot(gam_model)`) for describing our results and we can also make inference from predicted values.

For parametric variables, we can make inferences like we normally would. 

GAMs are especially useful for accounting for a non-linear phenomenon that may not be the main thing you are interested in. This is similar to how we have already used random effects to account for correlation in our data that is not the main thing of interest but shouldn't be ignored.

---

## Conclusions

- GAMs are GLMMs
- Anything you can do with LMs/LMMs/GLMs/GLMMs you can do with GAMs  
    - All the different ways we learned to deal with response functions for GLM and GLMM apply here too
- use ML to estimate parameters
- use a lot of knots

### Other GAMs things we won't get to look at in detail in this course

- changing basis functions (see a list of options by running `?smooth.terms`)

---
class: inverse, middle

# See you Wednesday for our last class!

.header[<u>[Go to topics list](#topics)</u>]

```{r eval = FALSE, echo = FALSE}

pagedown::chrome_print("sta303_w11_slides.html", wait = 10)

```
